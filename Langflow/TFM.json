{"id":"a56b3953-4b12-460b-8477-6dc2019651c6","data":{"nodes":[{"id":"ChatInput-Ry6te","type":"genericNode","position":{"x":-910.2310615533922,"y":-182.00421343068092},"data":{"type":"ChatInput","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"name":"files","value":"","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file","_input_type":"FileInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"Give me a breakdown of the Operating & Administ. Expenses of budget 2024 in MWC","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"User","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"User","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","files"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"ChatInput-Ry6te"},"selected":false,"width":384,"height":302,"dragging":false,"positionAbsolute":{"x":-910.2310615533922,"y":-182.00421343068092}},{"id":"ChatOutput-FOvY5","type":"genericNode","position":{"x":2784.291486630592,"y":-162.8738140687206},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"ChatOutput-FOvY5"},"selected":true,"width":384,"height":302,"positionAbsolute":{"x":2784.291486630592,"y":-162.8738140687206},"dragging":false},{"id":"Prompt-KTQGM","type":"genericNode","position":{"x":-166.79655391804135,"y":-1078.2524319691488},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Retrieve data for the following user query: {query}\n\n# Basic instructions\n1. You have to generate a sql sentence and a small description in order to show all of the necessary data to give another ai to answer the same query\n2. Be careful with null values and divisions by 0\n\n# Schemas\nSchemas definition for a postgresql database:\nCREATE TYPE ORG_ID_TYPE AS ENUM ('MWC', 'MWF', 'MSW');\nCREATE TYPE VALUE_TYPE AS ENUM ('REALIZED', 'ESTIMATED', 'FORECAST', 'BUDGETED');\nCREATE TYPE QUARTER_TYPE AS ENUM ('Q1', 'Q2', 'Q3', 'Q4');\n\nCREATE TABLE BUDGET6 (\n\tid SERIAL PRIMARY KEY,\n\tyear INT,           \t\t\t\t\t\t-- eg. 2023\n\tquarter QUARTER_TYPE,       -- Null if it corresponds to the entire year     \t\n\tcategory VARCHAR(128) NOT NULL,     \t\t-- e.g., \"Net Sales\", \"Gross Margin\"\n\tparent_category VARCHAR(128),            \t-- e.g., \"Revenues Total\", NULL for main categories\n\tvalue float8,                   \t-- Financial values\n\tis_percentage BOOLEAN DEFAULT FALSE,     \t-- True for percentage rows\n\tvalue_type VALUE_TYPE DEFAULT 'REALIZED', -- 'REALIZED', 'ESTIMATED', 'FORECAST', 'BUDGETED'\n\torg_id ORG_ID_TYPE DEFAULT 'MWC'\n);\n\n# Guidelines\n## Avaliable periods\n2023, None, REALIZED\n2024, \"Q1\", REALIZED\n2024, \"Q2\", REALIZED\n2024, \"Q1\", ESTIMATED\n2024, \"Q2\", ESTIMATED\n2024, \"Q3\", ESTIMATED\n2024, None, ESTIMATED\n2024, None, BUDGETED\n2025, None, REALIZED\n2025, None, BUDGETED\nSo including value_type will be necessary sometimes.\n\n## Category distinct fields\nOutsourced Employees Expenses Included in Personnel\nCommercial GM%\nManagement Commission\nOffice Exp.& Maintenance\nExpenses w/o Prov. Management &  Tax\nTransport & Car repairs\nFinancial cost on lease liabilities\nNon-Operating Revenues\nEntertainment\nStock deterioration\nTotal Sales\nWarranty and maintenance for BG\nHardware & Software Mainten.\nExpenses / Revenues %\nNet Income before provisions\nAudit Fees\nTuition Fees\nTop Up Tax (TUT)\nSundries\nConsulting Fees & Commissions\nTotal Expenses\nSponsor Fees\nIncome Taxes (Including the TUT)\nVacation-Air Tickets\nParticipation Revenue\nEBT\nForeigners Employees Included in Personnel\n%GM\nAccounting Charges\nSold Fixed Assets/Exceptional\nLocal Employees Included in Personnel\nTransportation Allowance\nRevenues Total\nAttorney Fees\nTraining & Seminars\nApartments Allowances\nAfter Sales Local\nExpenses w/o Mgt & Tax Variation % between YOY\nExpenses w/o Mgt & Tax/ GM %\nNet Income / HC\nSales Local\nIncome Taxes\nLocalization\nTotal Administrative\nOther Provision\nTotal Outsourcing\nManagement Fees\nTelephone Telex Fax & Courier\nCost of Sales\nNet Sales\nRent Office ,Warehouse & Cars/ Right of use\nITQAN Program Contribution to personnel Expenses (KSA)\nTotal Revenues / HC\nPersonnel Expenses\nCommissions & Bonus\nOther Allowance\nDiscounts covered by BDF Program\nGifts & Promotional Items\nLegal Fees & Taxes (Incl. WHT)\nHuman Resources (Inc. local program Emp.)\nHuman Resources (Excl. local program Emp.)\nNet Income in US$\nTotal Professional Services\nSub-Total Locals\nMarketing Contribution\nAfter Sales\nExchange Income\nBad Debts & Provisions\nFinancial Expenses\nExpenses / GM %\nITQAN program Employees Included in Personnel (KSA)\nBonus & Commission / GM %\nImpairment of Goodwill\nTools, Spare parts & Repair\nSales commission to other\nExceptional Loss\nOutsourcing Local\nFactoring\nCertification\nSubscription & Magazines\nOperating & Administ. Expenses\nInsurance Office & Cars\nMedical Insurance\nNet Income / Revenues %\nOperating Revenues\nNet Marketing\nFreight Clearing & Customs\nHead office expenses\nHC Recharged by Affiliates\nOutsourcing/Inside Sales\nAdministrative Local\nEBIT\nECL for Banks\nSub-Total Foreigners\nExchange Expenses\nTrading Portfolio Loss\nPrinting & Stationary\nGross Margin\nSalaries\nPenality Charges\nEnd of Service Indemnity\nProfessional Services Local\nExpenses w/o Prov. ,Mgt, tax & Outsourced included Personnel Exp\nAdvance Billing Provision\nElectricity & Water\nSales\nFinancial Revenues & Cash Discount\nAmortization & Depreciation\nTender fees\nOutsourced Employees Expenses Allocated to COS\nTotal After Sales\nSocial Security\nHC Recharged to Affiliates\nExhibition & Advertis.Expenses\nProf. Services\nAdministrative\nTravel Expenses\n\nFrom the list, the most relevant fields to extract conclusions usually are:\n'Revenues Total', 'Personnel Expenses', 'Gross Margin', 'Personnel Expenses', 'Operating & Administ. Expenses', 'Income Taxes (Including the TUT)', 'Total Expenses', 'Net Income in US$'\nSo extract them if asked.\n\n# Examples\n1. Query: \"Give me the 24/B-24 ratio in percentage for 2024 for the business MWC\"\nExpected response:\n\nsql: \"SELECT \n\ta.category, a.parent_category\n    CASE\n        WHEN b.value <> 0 THEN (a.value / b.value) * 100\n        ELSE NULL\n    END AS 24/B-24\nFROM \n    budget6 a\nJOIN \n    budget6 b\nON \n    a.categoryname = b.categoryname and a.quarterlabel is null and b.quarterlabel is null\nWHERE \n    a.period = '2024' AND\n    b.period = '2024' AND\n    a.valuetype = 'ESTIMATED' AND\n    b.valuetype = 'BUDGETED';\"\nshort_description: \"Estimated 2024/Budgeted 2024 ratio in percentage for MWC\"\n2. Query: \"Give me for the most relevant values the B.2025 - Est2024 for MWC\"\nsql: \"SELECT \n    a.category, \n    CASE\n        WHEN b.value IS NOT NULL AND b.value <> 0 THEN (a.value - b.value)\n        ELSE NULL\n    END AS \"B.2025 - Est2024\"\nFROM \n    budget6 a\nJOIN \n    budget6 b\nON \n    a.category = b.category AND a.quarter IS NULL AND b.quarter IS NULL\nWHERE \n    a.year = 2025 AND\n    b.year = 2024 AND\n    a.value_type = 'BUDGETED' AND\n    b.value_type = 'ESTIMATED' AND\n    a.org_id = 'MWC' AND\n    b.org_id = 'MWC' AND\n    a.category IN ('Revenues Total', 'Personnel Expenses', 'Gross Margin', 'Personnel Expenses', 'Operating & Administ. Expenses', 'Income Taxes (Including the TUT)', 'Total Expenses', 'Net Income in US$');\"\nshort_description: \"B.2025 - Est2024 for key financial categories for MWC.\"\n3. Query: \"Give me only the relevant figures for the budget 2024 and budget 2025 and interpret them as a financial expert\"\nsql: SELECT year, category, parent_category, value, value_type FROM budget6 WHERE quarter is null and year IN (2024, 2025) AND value_type = 'BUDGETED' AND org_id = 'MWC' AND category IN ('Revenues Total', 'Personnel Expenses', 'Gross Margin', 'Personnel Expenses', 'Operating & Administ. Expenses', 'Income Taxes (Including the TUT)', 'Total Expenses', 'Net Income in US$');\nshort_description: \"Budgeted values for 2024 and 2025 for key categories for MWC\"\n\n3. Query: \"Give me a breakdown of the Personnel expenses of budget 2024 in MWC\"\nsql: SELECT category, parent_category, value \n\tFROM budget6 \n\tWHERE \n\tyear = 2024 AND \n\tquarter IS NULL AND \n\tvalue_type = 'BUDGETED' AND \n\torg_id = 'MWC' \n\tAND (category = 'Personnel Expenses' or parent_category = 'Personnel Expenses');\nshort_description: \"Breakdown of Personnel Expenses for budget 2024 in MWC and Personnel Espenses Total\"\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"query":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"query","display_name":"query","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["query"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-KTQGM"},"selected":false,"width":384,"height":416,"dragging":false,"positionAbsolute":{"x":-166.79655391804135,"y":-1078.2524319691488}},{"id":"Prompt-VCV6S","type":"genericNode","position":{"x":1895.2066707902413,"y":256.9427368696091},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Answer the following query: {query}\n\nGuidelines:\n1. Units are in USD\n2. MWC, MWF, MSW are organization names\n\nUsing the follwing content if necessary:\n{content}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"query":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"query","display_name":"query","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"content":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"content","display_name":"content","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["query","content"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.18"},"id":"Prompt-VCV6S"},"selected":false,"width":384,"height":502,"dragging":false,"positionAbsolute":{"x":1895.2066707902413,"y":256.9427368696091}},{"id":"SQL Retriever-bmFil","type":"genericNode","position":{"x":712.7968817953738,"y":-1075.9266162613244},"data":{"type":"SQL Retriever","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output, DataInput\nfrom langflow.schema.message import Message\nimport psycopg2\nfrom tabulate import tabulate  # For generating Markdown tables\nfrom langflow.schema import Data\nprint(\"hi\")\nclass TextInputComponent(Component):\n    display_name = \"SQL Retriever\"\n    description = \"SQL Retriever\"\n    icon = \"database\"\n    name = \"SQL Retriever\"\n\n    inputs = [\n        MessageTextInput(name=\"host\", display_name=\"Host\"),\n        MessageTextInput(name=\"user\", display_name=\"User\"),\n        MessageTextInput(name=\"sql_query\", display_name=\"SQL Query\"),\n        MessageTextInput(name=\"short_description\", display_name=\"Short Description\", value=\"\"),\n        SecretStrInput(name=\"password\", display_name=\"Password\"),\n        MessageTextInput(name=\"database\", display_name=\"Database\"),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        try:\n            connection = psycopg2.connect(\n                host=self.host, # \"dev-maind.cnkw2weg8a5t.eu-west-1.rds.amazonaws.com\",\n                user=self.user, # postgres\n                password=self.password, # \"maind|159074-{RDS}\",\n                database=self.database # \"MIDIS\"\n            )\n            cursor = connection.cursor()\n\n            print(str(self.sql_query))\n            # Execute the query\n            cursor.execute(\n                str(self.sql_query)\n            )\n            result = cursor.fetchall()\n    \n            # Dynamically fetch headers\n            headers = [desc[0] for desc in cursor.description]\n            print(headers)\n    \n            # Generate Markdown table\n            markdown_table = tabulate(result, headers, tablefmt=\"pipe\", disable_numparse=True)\n    \n            # Clean up resources\n            cursor.close()\n            connection.close()\n            print(result)\n            print(markdown_table)\n            final_string = f\"{str(self.short_description)}\\n{markdown_table}\"\n            self.status = final_string\n            return Message(text=final_string)\n        except Exception as e:\n            return Message(text=\"Error retrieving data\")\n\n        ","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"database":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"database","value":"test","display_name":"Database","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"host":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"host","value":"","display_name":"Host","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"password":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"password","value":"","display_name":"Password","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"short_description":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"short_description","value":"","display_name":"Short Description","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sql_query":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sql_query","value":"","display_name":"SQL Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"user":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"user","value":"postgres","display_name":"User","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"SQL Retriever","icon":"database","base_classes":["Message"],"display_name":"SQL Retriever","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["host","user","sql_query","short_description","password","database"],"beta":false,"edited":true,"lf_version":"1.0.18"},"id":"SQL Retriever-bmFil"},"selected":false,"width":384,"height":732,"positionAbsolute":{"x":712.7968817953738,"y":-1075.9266162613244},"dragging":false},{"id":"OpenAIModel-ORjBN","type":"genericNode","position":{"x":286.8120173230395,"y":-1080.6187776138365},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o-mini","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed"],"beta":false,"edited":false},"id":"OpenAIModel-ORjBN"},"selected":false,"width":384,"height":605,"positionAbsolute":{"x":286.8120173230395,"y":-1080.6187776138365},"dragging":false},{"id":"OpenAIModel-uQ989","type":"genericNode","position":{"x":2366.474201348903,"y":-487.8776959500248},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":"0.3","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed"],"beta":false,"edited":false},"id":"OpenAIModel-uQ989"},"selected":false,"width":384,"height":605,"positionAbsolute":{"x":2366.474201348903,"y":-487.8776959500248},"dragging":false},{"id":"Prompt-qX8jv","type":"genericNode","position":{"x":590.3281859289318,"y":-133.09807945062812},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Retrieve data for the following user query: {query}\n\n# Basic instructions\n1. You have to generate a sql sentence and a small description in order to show all of the necessary data to give another ai to answer the same query\n2. Be careful with null values and divisions by 0\n\n# Schemas\nSchemas definition for a postgresql database:\nCREATE TYPE ORG_ID_TYPE AS ENUM ('MWC', 'MWF', 'MSW');\nCREATE TYPE VALUE_TYPE AS ENUM ('REALIZED', 'ESTIMATED', 'FORECAST', 'BUDGETED');\nCREATE TYPE QUARTER_TYPE AS ENUM ('Q1', 'Q2', 'Q3', 'Q4');\n\nCREATE TABLE BUDGET6 (\n\tid SERIAL PRIMARY KEY,\n\tyear INT,           \t\t\t\t\t\t-- eg. 2023\n\tquarter QUARTER_TYPE,       -- Null if it corresponds to the entire year     \t\n\tcategory VARCHAR(128) NOT NULL,     \t\t-- e.g., \"Net Sales\", \"Gross Margin\"\n\tparent_category VARCHAR(128),            \t-- e.g., \"Revenues Total\", NULL for main categories\n\tvalue float8,                   \t-- Financial values\n\tis_percentage BOOLEAN DEFAULT FALSE,     \t-- True for percentage rows\n\tvalue_type VALUE_TYPE DEFAULT 'REALIZED', -- 'REALIZED', 'ESTIMATED', 'FORECAST', 'BUDGETED'\n\torg_id ORG_ID_TYPE DEFAULT 'MWC'\n);\n\n# Guidelines\n## Avaliable periods\n2023, None, REALIZED\n2024, \"Q1\", REALIZED\n2024, \"Q2\", REALIZED\n2024, \"Q1\", ESTIMATED\n2024, \"Q2\", ESTIMATED\n2024, \"Q3\", ESTIMATED\n2024, None, ESTIMATED\n2024, None, BUDGETED\n2025, None, REALIZED\n2025, None, BUDGETED\nSo including value_type will be necessary sometimes.\n\n## Category distinct fields\nOutsourced Employees Expenses Included in Personnel\nCommercial GM%\nManagement Commission\nOffice Exp.& Maintenance\nExpenses w/o Prov. Management &  Tax\nTransport & Car repairs\nFinancial cost on lease liabilities\nNon-Operating Revenues\nEntertainment\nStock deterioration\nTotal Sales\nWarranty and maintenance for BG\nHardware & Software Mainten.\nExpenses / Revenues %\nNet Income before provisions\nAudit Fees\nTuition Fees\nTop Up Tax (TUT)\nSundries\nConsulting Fees & Commissions\nTotal Expenses\nSponsor Fees\nIncome Taxes (Including the TUT)\nVacation-Air Tickets\nParticipation Revenue\nEBT\nForeigners Employees Included in Personnel\n%GM\nAccounting Charges\nSold Fixed Assets/Exceptional\nLocal Employees Included in Personnel\nTransportation Allowance\nRevenues Total\nAttorney Fees\nTraining & Seminars\nApartments Allowances\nAfter Sales Local\nExpenses w/o Mgt & Tax Variation % between YOY\nExpenses w/o Mgt & Tax/ GM %\nNet Income / HC\nSales Local\nIncome Taxes\nLocalization\nTotal Administrative\nOther Provision\nTotal Outsourcing\nManagement Fees\nTelephone Telex Fax & Courier\nCost of Sales\nNet Sales\nRent Office ,Warehouse & Cars/ Right of use\nITQAN Program Contribution to personnel Expenses (KSA)\nTotal Revenues / HC\nPersonnel Expenses\nCommissions & Bonus\nOther Allowance\nDiscounts covered by BDF Program\nGifts & Promotional Items\nLegal Fees & Taxes (Incl. WHT)\nHuman Resources (Inc. local program Emp.)\nHuman Resources (Excl. local program Emp.)\nNet Income in US$\nTotal Professional Services\nSub-Total Locals\nMarketing Contribution\nAfter Sales\nExchange Income\nBad Debts & Provisions\nFinancial Expenses\nExpenses / GM %\nITQAN program Employees Included in Personnel (KSA)\nBonus & Commission / GM %\nImpairment of Goodwill\nTools, Spare parts & Repair\nSales commission to other\nExceptional Loss\nOutsourcing Local\nFactoring\nCertification\nSubscription & Magazines\nOperating & Administ. Expenses\nInsurance Office & Cars\nMedical Insurance\nNet Income / Revenues %\nOperating Revenues\nNet Marketing\nFreight Clearing & Customs\nHead office expenses\nHC Recharged by Affiliates\nOutsourcing/Inside Sales\nAdministrative Local\nEBIT\nECL for Banks\nSub-Total Foreigners\nExchange Expenses\nTrading Portfolio Loss\nPrinting & Stationary\nGross Margin\nSalaries\nPenality Charges\nEnd of Service Indemnity\nProfessional Services Local\nExpenses w/o Prov. ,Mgt, tax & Outsourced included Personnel Exp\nAdvance Billing Provision\nElectricity & Water\nSales\nFinancial Revenues & Cash Discount\nAmortization & Depreciation\nTender fees\nOutsourced Employees Expenses Allocated to COS\nTotal After Sales\nSocial Security\nHC Recharged to Affiliates\nExhibition & Advertis.Expenses\nProf. Services\nAdministrative\nTravel Expenses\n\nFrom the list, the most relevant fields to extract conclusions usually are:\n'Revenues Total', 'Personnel Expenses', 'Gross Margin', 'Personnel Expenses', 'Operating & Administ. Expenses', 'Income Taxes (Including the TUT)', 'Total Expenses', 'Net Income in US$'\nSo extract them if asked.\n\n# Examples\n1. Query: \"Give me the 24/B-24 ratio in percentage for 2024 for the business MWC\"\nExpected response:\n\nsql: \"SELECT \n\ta.category, a.parent_category\n    CASE\n        WHEN b.value <> 0 THEN (a.value / b.value) * 100\n        ELSE NULL\n    END AS 24/B-24\nFROM \n    budget6 a\nJOIN \n    budget6 b\nON \n    a.categoryname = b.categoryname and a.quarterlabel is null and b.quarterlabel is null\nWHERE \n    a.period = '2024' AND\n    b.period = '2024' AND\n    a.valuetype = 'ESTIMATED' AND\n    b.valuetype = 'BUDGETED';\"\nshort_description: \"Estimated 2024/Budgeted 2024 ratio in percentage for MWC\"\n2. Query: \"Give me for the most relevant values the B.2025 - Est2024 for MWC\"\nsql: \"SELECT \n    a.category, \n    CASE\n        WHEN b.value IS NOT NULL AND b.value <> 0 THEN (a.value - b.value)\n        ELSE NULL\n    END AS \"B.2025 - Est2024\"\nFROM \n    budget6 a\nJOIN \n    budget6 b\nON \n    a.category = b.category AND a.quarter IS NULL AND b.quarter IS NULL\nWHERE \n    a.year = 2025 AND\n    b.year = 2024 AND\n    a.value_type = 'BUDGETED' AND\n    b.value_type = 'ESTIMATED' AND\n    a.org_id = 'MWC' AND\n    b.org_id = 'MWC' AND\n    a.category IN ('Revenues Total', 'Personnel Expenses', 'Gross Margin', 'Personnel Expenses', 'Operating & Administ. Expenses', 'Income Taxes (Including the TUT)', 'Total Expenses', 'Net Income in US$');\"\nshort_description: \"B.2025 - Est2024 for key financial categories for MWC.\"\n3. Query: \"Give me only the relevant figures for the budget 2024 and budget 2025 and interpret them as a financial expert\"\nsql: SELECT year, category, parent_category, value, value_type FROM budget6 WHERE quarter is null and year IN (2024, 2025) AND value_type = 'BUDGETED' AND org_id = 'MWC' AND category IN ('Revenues Total', 'Personnel Expenses', 'Gross Margin', 'Personnel Expenses', 'Operating & Administ. Expenses', 'Income Taxes (Including the TUT)', 'Total Expenses', 'Net Income in US$');\nshort_description: \"Budgeted values for 2024 and 2025 for key categories for MWC\"\n\n3. Query: \"Give me a breakdown of the Personnel expenses of budget 2024 in MWC\"\nsql: SELECT category, parent_category, value \n\tFROM budget6 \n\tWHERE \n\tyear = 2024 AND \n\tquarter IS NULL AND \n\tvalue_type = 'BUDGETED' AND \n\torg_id = 'MWC' \n\tAND (category = 'Personnel Expenses' or parent_category = 'Personnel Expenses');\nshort_description: \"Breakdown of Personnel Expenses for budget 2024 in MWC and Personnel Espenses Total\"\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"query":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"query","display_name":"query","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["query"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-qX8jv"},"selected":false,"width":384,"height":416,"positionAbsolute":{"x":590.3281859289318,"y":-133.09807945062812},"dragging":false},{"id":"OpenAIModel-fGO61","type":"genericNode","position":{"x":1037.827812821686,"y":-134.75351837141926},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o-mini","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed"],"beta":false,"edited":false},"id":"OpenAIModel-fGO61"},"selected":false,"width":384,"height":605,"positionAbsolute":{"x":1037.827812821686,"y":-134.75351837141926},"dragging":false},{"id":"SQL Retriever-mBBho","type":"genericNode","position":{"x":1481.664026325779,"y":-137.6355717058615},"data":{"type":"SQL Retriever","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output, DataInput\nfrom langflow.schema.message import Message\nimport psycopg2\nfrom tabulate import tabulate  # For generating Markdown tables\nfrom langflow.schema import Data\nprint(\"hi\")\nclass TextInputComponent(Component):\n    display_name = \"SQL Retriever\"\n    description = \"SQL Retriever\"\n    icon = \"database\"\n    name = \"SQL Retriever\"\n\n    inputs = [\n        MessageTextInput(name=\"host\", display_name=\"Host\"),\n        MessageTextInput(name=\"user\", display_name=\"User\"),\n        MessageTextInput(name=\"sql_query\", display_name=\"SQL Query\"),\n        MessageTextInput(name=\"short_description\", display_name=\"Short Description\", value=\"\"),\n        SecretStrInput(name=\"password\", display_name=\"Password\"),\n        MessageTextInput(name=\"database\", display_name=\"Database\"),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        try:\n            connection = psycopg2.connect(\n                host=self.host, # \"dev-maind.cnkw2weg8a5t.eu-west-1.rds.amazonaws.com\",\n                user=self.user, # postgres\n                password=self.password, # \"maind|159074-{RDS}\",\n                database=self.database # \"MIDIS\"\n            )\n            cursor = connection.cursor()\n\n            print(str(self.sql_query))\n            # Execute the query\n            cursor.execute(\n                str(self.sql_query)\n            )\n            result = cursor.fetchall()\n    \n            # Dynamically fetch headers\n            headers = [desc[0] for desc in cursor.description]\n            print(headers)\n    \n            # Generate Markdown table\n            markdown_table = tabulate(result, headers, tablefmt=\"pipe\", disable_numparse=True)\n    \n            # Clean up resources\n            cursor.close()\n            connection.close()\n            print(result)\n            print(markdown_table)\n            final_string = f\"{str(self.short_description)}\\n{markdown_table}\"\n            self.status = final_string\n            return Message(text=final_string)\n        except Exception as e:\n            return Message(text=\"Error retrieving data\")\n\n        ","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"database":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"database","value":"test","display_name":"Database","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"host":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"host","value":"","display_name":"Host","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"password":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"password","value":"","display_name":"Password","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"short_description":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"short_description","value":"","display_name":"Short Description","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sql_query":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sql_query","value":"","display_name":"SQL Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"user":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"user","value":"postgres","display_name":"User","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"SQL Retriever","icon":"database","base_classes":["Message"],"display_name":"SQL Retriever","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["host","user","sql_query","short_description","password","database"],"beta":false,"edited":true,"lf_version":"1.0.18"},"id":"SQL Retriever-mBBho"},"selected":false,"width":384,"height":732,"positionAbsolute":{"x":1481.664026325779,"y":-137.6355717058615},"dragging":false}],"edges":[{"source":"ChatInput-Ry6te","target":"Prompt-KTQGM","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-Ry6teœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œqueryœ,œidœ:œPrompt-KTQGMœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-ChatInput-Ry6te{œdataTypeœ:œChatInputœ,œidœ:œChatInput-Ry6teœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-KTQGM{œfieldNameœ:œqueryœ,œidœ:œPrompt-KTQGMœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"query","id":"Prompt-KTQGM","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-Ry6te","name":"message","output_types":["Message"]}},"selected":false,"className":""},{"source":"ChatInput-Ry6te","target":"Prompt-VCV6S","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-Ry6teœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œqueryœ,œidœ:œPrompt-VCV6Sœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-ChatInput-Ry6te{œdataTypeœ:œChatInputœ,œidœ:œChatInput-Ry6teœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-VCV6S{œfieldNameœ:œqueryœ,œidœ:œPrompt-VCV6Sœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"query","id":"Prompt-VCV6S","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-Ry6te","name":"message","output_types":["Message"]}},"selected":false,"className":""},{"source":"Prompt-KTQGM","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-KTQGMœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-ORjBN","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-ORjBNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-ORjBN","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-KTQGM","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-KTQGM{œdataTypeœ:œPromptœ,œidœ:œPrompt-KTQGMœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-ORjBN{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-ORjBNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"OpenAIModel-ORjBN","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-ORjBNœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"SQL Retriever-bmFil","targetHandle":"{œfieldNameœ:œsql_queryœ,œidœ:œSQL Retriever-bmFilœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"sql_query","id":"SQL Retriever-bmFil","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-ORjBN","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-ORjBN{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-ORjBNœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-SQL Retriever-bmFil{œfieldNameœ:œsql_queryœ,œidœ:œSQL Retriever-bmFilœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"Prompt-VCV6S","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-VCV6Sœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-uQ989","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-uQ989œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-uQ989","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-VCV6S","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-VCV6S{œdataTypeœ:œPromptœ,œidœ:œPrompt-VCV6Sœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-uQ989{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-uQ989œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"OpenAIModel-uQ989","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-uQ989œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-FOvY5","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-FOvY5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-FOvY5","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-uQ989","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-uQ989{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-uQ989œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-FOvY5{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-FOvY5œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"SQL Retriever-bmFil","sourceHandle":"{œdataTypeœ:œSQL Retrieverœ,œidœ:œSQL Retriever-bmFilœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-qX8jv","targetHandle":"{œfieldNameœ:œqueryœ,œidœ:œPrompt-qX8jvœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"query","id":"Prompt-qX8jv","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"SQL Retriever","id":"SQL Retriever-bmFil","name":"text","output_types":["Message"]}},"id":"reactflow__edge-SQL Retriever-bmFil{œdataTypeœ:œSQL Retrieverœ,œidœ:œSQL Retriever-bmFilœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-qX8jv{œfieldNameœ:œqueryœ,œidœ:œPrompt-qX8jvœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"},{"source":"Prompt-qX8jv","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-qX8jvœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-fGO61","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-fGO61œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-fGO61","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-qX8jv","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-qX8jv{œdataTypeœ:œPromptœ,œidœ:œPrompt-qX8jvœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-fGO61{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-fGO61œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"OpenAIModel-fGO61","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-fGO61œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"SQL Retriever-mBBho","targetHandle":"{œfieldNameœ:œsql_queryœ,œidœ:œSQL Retriever-mBBhoœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"sql_query","id":"SQL Retriever-mBBho","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-fGO61","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-fGO61{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-fGO61œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-SQL Retriever-mBBho{œfieldNameœ:œsql_queryœ,œidœ:œSQL Retriever-mBBhoœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"SQL Retriever-mBBho","sourceHandle":"{œdataTypeœ:œSQL Retrieverœ,œidœ:œSQL Retriever-mBBhoœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-VCV6S","targetHandle":"{œfieldNameœ:œcontentœ,œidœ:œPrompt-VCV6Sœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"content","id":"Prompt-VCV6S","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"SQL Retriever","id":"SQL Retriever-mBBho","name":"text","output_types":["Message"]}},"id":"reactflow__edge-SQL Retriever-mBBho{œdataTypeœ:œSQL Retrieverœ,œidœ:œSQL Retriever-mBBhoœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-VCV6S{œfieldNameœ:œcontentœ,œidœ:œPrompt-VCV6Sœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"}],"viewport":{"x":429.4227720003763,"y":421.0030116990956,"zoom":0.3508118946693664}},"description":"Promptly Ingenious!","name":"TFM","last_tested_version":"1.0.18","endpoint_name":null,"is_component":false}